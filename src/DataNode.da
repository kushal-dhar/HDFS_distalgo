"""
Datanode process in HDFS cluster
"""

#NameNode = import_da('NameNode')

import Config
import os
import time
from threading import Thread

'''
# send() did not work in Thread. Had to create process for it to work
class DataNodeHeartbeat(Thread):
    def __init__(self, datanode, namenode, interval=Config.DATANODE_HEARTBEAT_INTERVAL):
        Thread.__init__(self)
        self.datanode = datanode
        self.namenode = namenode
        self.interval = interval

    def run(self):
        while True:
            # output('sending heartbeat', self)
            send(('heartbeat', self.datanode), to=self.namenode)
            time.sleep(self.interval)
'''

class DataNodeHeartbeat(process):
    def setup(datanode, namenode, interval=Config.DATANODE_HEARTBEAT_INTERVAL):
        pass

    def run():
        output('start heartbeat process for datanode: ', datanode)
        while True:
            # output('sending heartbeat', self)
            send(('heartbeat', datanode), to=namenode)
            time.sleep(interval)

    def receive(msg=('kill')):
        output('received kill')
        exit(0)

class DataNode(process):
#class DataNode():


    """
    def __init__():
        nameNode = None
        # The namespace for which this datanode is storing data
        # A datanode can only store data for one namespace
        namespaceID = None
        namespaceIDPath = None
        # Used by namenode to uniquely identify datanodes
        storageID = None
        storageIDPath = None
    """ 

    def setup(namenodes, storageDir):
        # Declare all class variables with self. here to reference them without self throughout
        self.datanodeId = None
        self.heartbeatThread = None

    def run():
        output('datanode started: ', self)
        output('supplied namenode: ', namenodes)

        datanodeId = readDatanodeId()

        send(('add_datanode', self, datanodeId), to=namenodes)
        await(some(received(('datanode_added',_))))
        output('registered successfully with namenode')

        heartbeatProcess = new(DataNodeHeartbeat, num=1, at=Config.DATANODE_LOCATION)
        for hbp in heartbeatProcess:
            setup(hbp, args=(self, namenodes))
        start(heartbeatProcess)


        await(received(('kill')))
        output('received kill message')
        exit(0)

        
    def connectToNamenode(namenode):
        """
        Performs handshake and verification with namenode.
        After this passes, this datanode is available for storing data blocks.
        """
        pass
    
    def getNamespaceId():
        """
        Returns the namespace id for which this datanode is storing data
        """
        pass
    
    def setNamespaceId(id):
        """
        Sets the namespace id for which this datanode is storing data.
        This id is stored persistently on datanode.
        """
        pass

    def getStorageId():
        """
        Returns the storage id for which this datanode is storing data
        """
        pass
        
    def setStorageId(id):
        """
        Sets the storage id for which this datanode is storing data.
        This id is stored persistently on datanode.
        """
        pass
        
    def sendBlockReport():
        """
        Sends block report to namenode.
        Block reports are sent periodically to namenode.
        Block reports contain list of blocks of files this datanode is storing.
        """
        pass
        
    def sendHeartBeat():
        """
        Sends heartbeat to namenode to signal that this datanode is alive
        """
        pass


    def readDatanodeId():
        filepath = os.path.join(storageDir, Config.DATANODE_ID_FILE)
        if not os.path.isfile(filepath):
            return None
        fid = open(filepath, 'r')
        data = fid.readline()
        datanodeId = int(data)
        fid.close()
        return datanodeId

    def writeDatanodeId():
        filepath = os.path.join(storageDir, Config.DATANODE_ID_FILE)
        fid = open(filepath, 'w')
        # Possible issue: str(None)
        pid = str(self)
        output(pid, ' -> ', datanodeId)
        fid.write(str(datanodeId) + '\n' + pid)
        fid.close()

    def receive(msg=('datanode_added', proposedId)):
        if proposedId != datanodeId:
            datanodeId = proposedId
            writeDatanodeId()



    def sendHeartbeat():
        send(('heartbeat', self), to=namenodes)


    def receive(msg=('readFile_c2d', client, filename, nbytes, messageId)):
        output('datanode: received readFile filename={0}, nbytes={1}'.format(filename, nbytes))
        # TODO : change mode to 'rb'
        #fid = open(filename, 'r')
        filepath = os.path.join(self.storageDir, filename)
        fid = open(filepath, 'rb')
        # TODO : change to data = fid.read(nbytes)
        #data = fid.read()
        data = fid.read(nbytes)

        output('datanode: output of readFile filename={0}, nbytes={1}: data={2}'.format(filename, nbytes, data))
        send(('readFile_d2c', filename, nbytes, messageId, data), to=client)


    def receive(msg=('appendFile_c2d', filename, blockName, blockNumber, data, datanodes_to_write, messageIdTuple, idx, client), from_=sender):
        filepath = os.path.join(self.storageDir, blockName)
        output('received appendFile for filename={0}, filepath={1}'.format(filename, filepath))
        # TODO- change to wb afterwards
        fid = open(filepath, 'a+')
        fid.write(data)
        fid.close()
        if idx == len(datanodes_to_write) - 1:
            pass
        else:
            # Pipeline fashion
            send(('appendFile_c2d', filename, blockName, blockNumber, data, datanodes_to_write, messageIdTuple, idx+1, client), to=datanodes_to_write[idx+1])
        # TODO - incorporate block number in these messages
        bytesWritten = len(data)
        send(('appendFile_d2n', filename, blockNumber, bytesWritten, self.datanodeId), to=namenodes)
        send(('appendFile_d2c', messageIdTuple[idx]), to=client)


    def receive(msg=('kill')):
        output('handler: received kill message')
        exit(0)

"""
Datanode interacts with HDFS Client using DatanodeClient
"""
class DatanodeClient:
    
    def __init__():
        pass
    
    def readBlock(filepath):
        """
        Reads the bytes from the specified file.
        Also returns the checksum to check for possible corruption
        """
        pass
    
    def writeBlock(filepath, data):
        """
        Writes the data to specified file.
        Updates access time, modified time and checksum for the accessed block
        """
        pass
    
    def writeDataPipelined(filepath, data, nextDatanode):
        """
        Writes the data in pipelined fashion.
        Writes the data to local file and also sends the data to next node in the pipeline
        """
        pass
